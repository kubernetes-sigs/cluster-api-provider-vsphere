# This yaml deploys the autoscaler on a workload cluster and configures it to match
# against the corresponding Cluster API cluster which is defined into the management cluster.
---
# Specify kubeconfig for management cluster
apiVersion: v1
kind: Secret
metadata:
  name: kubeconfig-management-cluster
  namespace: ${CLUSTER_NAMESPACE}
stringData:
  kubeconfig: |
    apiVersion: v1
    kind: Config
    clusters:
    - name: management-cluster
      cluster:
        certificate-authority-data: ${MANAGEMENT_CLUSTER_CA}
        server: ${MANAGEMENT_CLUSTER_ADDRESS}
    contexts:
    - name: management-context
      context:
        cluster: management-cluster
        namespace: ${CLUSTER_NAMESPACE}
        user: cluster-autoscaler-sa
    current-context: management-context
    users:
    - name: cluster-autoscaler-sa
      user:
        token: "${MANAGEMENT_CLUSTER_TOKEN}"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: ${CLUSTER_NAMESPACE}
  labels:
    app: cluster-autoscaler
spec:
  selector:
    matchLabels:
      app: cluster-autoscaler
  replicas: 1
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      containers:
        - image: gcr.io/k8s-staging-autoscaling/cluster-autoscaler:v20250814-cluster-autoscaler-chart-9.50.1-4-ga9cb59fdd
          name: cluster-autoscaler
          command:
            - /cluster-autoscaler
          args:
            - --cloud-provider=clusterapi
            # Specify kubeconfig for management cluster
            - --cloud-config=/management-cluster/kubeconfig
            # Specify kubeconfig for workload cluster
            - --kubeconfig=/workload-cluster/value
            # Limit cluster autoscaler to only match against resources belonging to a single Cluster API cluster
            - --node-group-auto-discovery=clusterapi:namespace=${CLUSTER_NAMESPACE},clusterName=${CLUSTER_NAME}
            # Set a short scale down unneeded time, so we don't have to wait too long during e2e testing
            - --scale-down-unneeded-time=1m
            #  Set a short scale down delay after add time, so we don't have to wait too long during e2e testing
            - --scale-down-delay-after-add=1m
            # Set a short scale down delay after delete time, so we don't have to wait too long during e2e testing
            - --scale-down-delay-after-delete=1m
            # Set a short scale down delay after failure time, so we don't have to wait too long during e2e testing
            - --scale-down-delay-after-failure=1m
            # Set a max nodes limit as safeguard so that the test does not scale up unbounded.
            # Note: The E2E test should only go up to 4 (assuming it starts with a min node group size of 2).
            # Using 6 for additional some buffer and to allow different starting min node group sizes.
            - --max-nodes-total=6
            - --v=4
          volumeMounts:
            - name: kubeconfig-management-cluster
              mountPath: /management-cluster
              readOnly: true
            - name: kubeconfig-workload-cluster
              mountPath: /workload-cluster
              readOnly: true
      terminationGracePeriodSeconds: 10
      volumes:
        - name: kubeconfig-management-cluster
          secret:
            secretName: kubeconfig-management-cluster
            optional: false
        - name: kubeconfig-workload-cluster
          secret:
            secretName: ${CLUSTER_NAME}-kubeconfig
            optional: false
